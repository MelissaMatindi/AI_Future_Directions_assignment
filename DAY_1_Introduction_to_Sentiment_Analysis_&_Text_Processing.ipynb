{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMh2ZxyRK0gkffMgyPgOYSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelissaMatindi/AI_Future_Directions_assignment/blob/main/DAY_1_Introduction_to_Sentiment_Analysis_%26_Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Day 1: Introduction to Sentiment Analysis & Text Processing**\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "* What is sentiment analysis? Applications in industry\n",
        "* Binary vs. multi-class sentiment classification\n",
        "* Text preprocessing pipeline: tokenization, stopword removal, stemming/lemmatization\n",
        "* Introduction to bag-of-words and TF-IDF representations\n",
        "\n",
        "**Hands-on Task:**\n",
        "1. Install required libraries: nltk, scikit-learn, pandas, numpy, matplotlib\n",
        "2. Create a simple text preprocessing function:\n",
        "3. Test on sample sentences with varying sentiment\n",
        "\n",
        "\n",
        "**Deliverable:** Python script with preprocessing function and test cases\n"
      ],
      "metadata": {
        "id": "RdKq7J6RYsbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THEORY:**\n",
        "-----\n",
        "---\n",
        "\n",
        "## **a) What is sentiment analysis?**\n",
        "\n",
        "Sentiment analysis is the use of NLP to identify, extract, and quantify subjective information ‚Äî primarily the emotional tone (positive, negative, neutral) expressed in text.\n",
        "\n",
        "Text is unstructured; so sentiment turns it into actionable insights.\n",
        "\n",
        "## **Applications in industry**\n",
        "\n",
        "* **Brand Monitoring:** Companies track mentions on X/Twitter or Reddit. Example: Coca-Cola monitors real-time sentiment during ad campaigns to detect backlash early.\n",
        "* **Customer Feedback:** Amazon/e-commerce analyzes product reviews to prioritize improvements.\n",
        "* **Social Media Crisis Detection:** NGOs or social platforms flag rising negative sentiment in user posts as early distress signals.\n",
        "* **Market Research:** Political campaigns gauge public opinion on candidates from news/forums.\n",
        "* **Financial Trading:** Hedge funds analyze earnings call transcripts or tweets for stock sentiment.\n",
        "\n",
        " During product launches, a sudden sentiment drop can trigger PR responses, saving millions.\n",
        "\n",
        "---\n",
        "\n",
        "## **b) Binary vs. multi-class sentiment classification**\n",
        "\n",
        "**Binary Classification:**\n",
        "\n",
        "- Two classes: Positive vs. Negative.\n",
        "- Simpler, more robust, higher accuracy.\n",
        "- Example: IMDB movie reviews (pos/neg).\n",
        "- Best when neutral is rare or irrelevant.\n",
        "\n",
        "**Multi-class Classification:**\n",
        "\n",
        "- Three or more classes: Positive, Negative, Neutral\n",
        "- More informative but harder (data imbalance, lower accuracy, needs more labeled data).\n",
        "- Example: Analyzing X posts for mental health ‚Äî detecting \"neutral\" vs. \"anxious\" vs. \"depressed\" tones.\n",
        "---\n",
        "\n",
        "## **c) Text preprocessing pipeline: tokenization, stopword removal, stemming/lemmatization**\n",
        "\n",
        "Raw text is noisy (punctuation, capitalization, common words). Preprocessing standardizes it so models focus on meaningful signals.\n",
        "Standard Steps:\n",
        "\n",
        "1. Lowercasing: \"Great\" ‚Üí \"great\" (reduces vocabulary size).\n",
        "2. Remove punctuation/numbers/URLs: \"Wow!!!\" ‚Üí \"Wow\".\n",
        "3. Tokenization: Split into words/tokens.\n",
        "Example: \"I love NLP!\" ‚Üí [\"I\", \"love\", \"NLP\"].\n",
        "4. Stopword Removal: Drop high-frequency, low-meaning words.\n",
        "\n",
        "*  English stopwords: \"the\", \"is\", \"and\", \"of\", \"to\".\n",
        "*  Example: \"the movie is great\" ‚Üí \"movie great\".\n",
        "\n",
        "5. Stemming or Lemmatization:\n",
        "* Stemming: Rule-based chopping (Porter/Snowball). Fast but crude.\n",
        "\"running\", \"runner\", \"ran\" ‚Üí \"run\".\n",
        "* Lemmatization: Context-aware (uses WordNet dictionary). Slower but accurate.\n",
        "\"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\", \"mice\" ‚Üí \"mouse\".\n",
        "\n",
        "---\n",
        "\n",
        "## **d) Introduction to bag-of-words and TF-IDF representations**\n",
        "\n",
        "**Bag-of-Words (BoW):**\n",
        "\n",
        "* Simplest: Represent text as a vector of word counts (ignores order).\n",
        "\n",
        "Example corpus:\n",
        "Doc1: \"great movie\"\n",
        "Doc2: \"terrible movie\"\n",
        "\n",
        "Vocabulary: [\"great\", \"movie\", \"terrible\"]\n",
        "\n",
        "BoW vectors:\n",
        "Doc1: [1, 1, 0]\n",
        "Doc2: [0, 1, 1]\n",
        "\n",
        "* Pros: Simple, interpretable.\n",
        "* Cons: No word importance or semantics.\n",
        "\n",
        "\n",
        "\n",
        "**TF-IDF (Term Frequency‚ÄìInverse Document Frequency):**\n",
        "\n",
        "* Improves BoW by weighting:\n",
        "* TF: How often a word appears in a document.\n",
        "* IDF: Reduces weight of common words across all documents.\n",
        "\n",
        "* Example: \"movie\" appears in both ‚Üí low IDF hence downweighted. \"great\"/\"terrible\" ‚Üí high IDF hence important.\n",
        "* Result: Rare sentiment-bearing words get higher scores.\n",
        "* Industry standard for classic classifiers like Naive Bayes."
      ],
      "metadata": {
        "id": "5MU_tPmRZvjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hands-On Tasks**"
      ],
      "metadata": {
        "id": "R-OKA2lKiSWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install required libraries: nltk, scikit-learn, pandas, numpy, matplotlib"
      ],
      "metadata": {
        "id": "QBax52P_icxg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1oPu5HgYdNa",
        "outputId": "78b86acc-9c03-4228-fe28-a6b15f5c5b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Installing libraries\n",
        "\n",
        "!pip install nltk scikit-learn pandas numpy matplotlib --quiet\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create a simple text preprocessing function"
      ],
      "metadata": {
        "id": "PQeX34vzi7rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "\n",
        "nltk.download('punkt')          # Tokenizer\n",
        "nltk.download('stopwords')      # English stopwords\n",
        "nltk.download('wordnet')        # Lemmatizer\n",
        "nltk.download('omw-1.4')        # Extended WordNet data\n",
        "\n",
        "print(\"NLTK data downloaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X9GTsJ-jXYB",
        "outputId": "472ab500-64bb-492a-b3e5-5f54fedc12a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK data downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Processing Function\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text,\n",
        "                    lowercase=True,\n",
        "                    remove_punct=True,\n",
        "                    remove_stopwords=True,\n",
        "                    lemmatize=True):\n",
        "    \"\"\"\n",
        "    Comprehensive text preprocessing function.\n",
        "    Parameters control each step for experimentation.\n",
        "    \"\"\"\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "\n",
        "    if remove_punct:\n",
        "        # Remove everything except letters and spaces\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    if lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Return as cleaned string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Function defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddMLYgwHj28M",
        "outputId": "30b22dc0-75dc-4403-dd5b-721bafda2f09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Test on sample sentences with varying sentiment"
      ],
      "metadata": {
        "id": "Kauf95b0jdHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Diverse sample sentences (movie reviews + social-style)\n",
        "samples = [\n",
        "    \"This movie was ABSOLUTELY AMAZING!!! I loved every second of it üòç\",\n",
        "    \"Terrible film. Complete waste of time and money.\",\n",
        "    \"It was okay, nothing special. Not bad, not great.\",\n",
        "    \"The acting was brilliant but the plot was so predictable...\",\n",
        "    \"Can't believe how bad this was!! Worst movie ever üëé\",\n",
        "    \"Not going to lie, I actually enjoyed it more than expected!\",\n",
        "    \"The visuals were stunning, but the story felt empty.\",\n",
        "    \"What a masterpiece! Everyone should watch this.\"\n",
        "]\n",
        "\n",
        "print(\"Original vs Cleaned:\\n\")\n",
        "for i, sentence in enumerate(samples, 1):\n",
        "    cleaned = preprocess_text(sentence)\n",
        "    print(f\"{i}. Original: {sentence}\")\n",
        "    print(f\"   Cleaned : {cleaned}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUHZunBPkWGW",
        "outputId": "852a50d2-c017-4f39-ca0e-82017822c173"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs Cleaned:\n",
            "\n",
            "1. Original: This movie was ABSOLUTELY AMAZING!!! I loved every second of it üòç\n",
            "   Cleaned : movie absolutely amazing loved every second\n",
            "\n",
            "2. Original: Terrible film. Complete waste of time and money.\n",
            "   Cleaned : terrible film complete waste time money\n",
            "\n",
            "3. Original: It was okay, nothing special. Not bad, not great.\n",
            "   Cleaned : okay nothing special bad great\n",
            "\n",
            "4. Original: The acting was brilliant but the plot was so predictable...\n",
            "   Cleaned : acting brilliant plot predictable\n",
            "\n",
            "5. Original: Can't believe how bad this was!! Worst movie ever üëé\n",
            "   Cleaned : cant believe bad worst movie ever\n",
            "\n",
            "6. Original: Not going to lie, I actually enjoyed it more than expected!\n",
            "   Cleaned : going lie actually enjoyed expected\n",
            "\n",
            "7. Original: The visuals were stunning, but the story felt empty.\n",
            "   Cleaned : visuals stunning story felt empty\n",
            "\n",
            "8. Original: What a masterpiece! Everyone should watch this.\n",
            "   Cleaned : masterpiece everyone watch\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT\n",
        "# Trying variations to see impact\n",
        "print(\"Experiment: Without lemmatization\")\n",
        "print(preprocess_text(samples[0], lemmatize=False))\n",
        "\n",
        "print(\"\\nExperiment: Keep stopwords\")\n",
        "print(preprocess_text(samples[0], remove_stopwords=False))\n",
        "\n",
        "print(\"\\nExperiment: Minimal cleaning (only lowercase + tokenize)\")\n",
        "print(preprocess_text(samples[0], remove_punct=False, remove_stopwords=False, lemmatize=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BnmYLDbktdG",
        "outputId": "519ed05e-5f8d-4cb8-872d-af53ffb36888"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment: Without lemmatization\n",
            "movie absolutely amazing loved every second\n",
            "\n",
            "Experiment: Keep stopwords\n",
            "this movie wa absolutely amazing i loved every second of it\n",
            "\n",
            "Experiment: Minimal cleaning (only lowercase + tokenize)\n",
            "this movie was absolutely amazing ! ! ! i loved every second of it üòç\n"
          ]
        }
      ]
    }
  ]
}